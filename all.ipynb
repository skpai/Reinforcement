{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, maze, epsilon, gamma, alpha, \n",
    "                 state, q_table, actions=['up', 'down', 'left', 'right']):\n",
    "        self.maze=maze\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = gamma  \n",
    "        self.alpha = alpha\n",
    "        self.state = state \n",
    "        self.actions = actions #  ['up', 'down', 'left', 'right']\n",
    "        self.q_table = q_table \n",
    "    \n",
    "    def choose_action(self):\n",
    "        state_actions = self.q_table[self.state]\n",
    "        randomRate = np.random.uniform()\n",
    "        if randomRate > self.epsilon:\n",
    "            return self.actions.index(np.random.choice(self.actions)) \n",
    "\n",
    "        else:\n",
    "            return state_actions.argmax()\n",
    "    \n",
    "    def update_q_table(self, reward, action, nxt_state):\n",
    "        q_predict = self.q_table[self.state][action]\n",
    "        if nxt_state in ['win', 'ghost'] :\n",
    "            q_target = reward\n",
    "        else:\n",
    "            q_target = reward + self.gamma * self.q_table[nxt_state][action]\n",
    "        self.q_table[self.state][action] += self.alpha * (q_target - q_predict) \n",
    "        self.state = nxt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        \n",
    "    def build_map(self, size, target, t_reward):\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        self.map = -1+np.zeros((size))\n",
    "        self.all_positions=[(i,j) for i in range(size[0]) for j in range(size[1])]\n",
    "        for item in target:\n",
    "            x,y=item\n",
    "            self.map[x,y] = t_reward\n",
    "        return\n",
    "    \n",
    "    def env_feedback(self, state, action):\n",
    "        nxt_state = self.cal_coordinate(state, action)\n",
    "        \n",
    "        reward = self.map[nxt_state]\n",
    "        if nxt_state in self.target:\n",
    "            nxt_state = 'win'\n",
    "            \n",
    "        return nxt_state, reward\n",
    "    \n",
    "    def cal_coordinate(self, state, action):\n",
    "        next_state = ()\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 2:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        if next_state not in self.all_positions:\n",
    "            next_state=state\n",
    "        return next_state\n",
    "    \n",
    "    def create_q_table(self):\n",
    "        q_table = np.zeros(self.size + (len(self.actions),))\n",
    "        print('Q_table.shape :')\n",
    "        print(q_table.shape)\n",
    "        return np.array(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 20\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "initSTATE = (0,0)\n",
    "SIZE = (5,5) # maze size\n",
    "\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [(3,1), (4,4)]\n",
    "t_reward = 100\n",
    "\n",
    "f_reward_list = -1+np.zeros(SIZE)\n",
    "Maze = Maze(ACTIONS)\n",
    "Maze.build_map(SIZE, target, t_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_table.shape :\n",
      "(5, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "Q_table = Maze.create_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = Agent(Maze,epsilon= EPSILON, gamma= GAMMA, alpha= ALPHA,\n",
    "             state = initSTATE, actions= ACTIONS,q_table= Q_table.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) > (1, 0) > (0, 0) > (0, 1) > (0, 1) > (1, 1) > (0, 1) > (0, 2) > (0, 2) > (1, 2) > (0, 2) > (0, 3) > (0, 3) > (1, 3) > (0, 3) > (0, 4) > (0, 4) > (1, 4) > (0, 4) > (0, 4) > (0, 3) > (0, 2) > (0, 1) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (1, 0) > (1, 1) > (1, 2) > (2, 2) > (1, 2) > (1, 3) > (2, 3) > (1, 3) > (1, 4) > (2, 4) > (1, 4) > (1, 4) > (1, 3) > (1, 2) > (1, 1) > (2, 1) > (1, 1) > (1, 0) > (1, 0) > (2, 0) > (3, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 0 finished ... ,total step : 52\n",
      "(0, 1) > (1, 1) > (2, 1) > win\n",
      " Episode. 1 finished ... ,total step : 4\n",
      "(0, 1) > (0, 2) > (1, 2) > (2, 2) > (3, 2) > (2, 2) > (2, 3) > (3, 3) > (2, 3) > (2, 4) > (3, 4) > (2, 4) > (2, 4) > (2, 3) > (2, 2) > (2, 1) > win\n",
      " Episode. 2 finished ... ,total step : 17\n",
      "(1, 0) > (1, 1) > (2, 1) > win\n",
      " Episode. 3 finished ... ,total step : 4\n",
      "(1, 0) > (0, 0) > (0, 1) > (0, 0) > (1, 0) > (2, 0) > (2, 0) > (3, 0) > (4, 0) > (3, 0) > win\n",
      " Episode. 4 finished ... ,total step : 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/tmp/ipykernel_1255171/1536227656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Episode. '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' finished ... ,total step : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def path(state, is_terminated):\n",
    "    print(state, end='')\n",
    "    if not is_terminated: print(' > ', end='')\n",
    "\n",
    "# main process RL - Q_Learning\n",
    "for episode in range(EPISODES):\n",
    "    Agent.state = initSTATE\n",
    "    is_terminated = False\n",
    "    count = 0\n",
    "    while not is_terminated :\n",
    "        action = Agent.choose_action()\n",
    "        nxt_state, reward = Maze.env_feedback(state=Agent.state, action=action)\n",
    "        if Agent.state == nxt_state: \n",
    "            reward = -10\n",
    "        ###########################################\n",
    "        Agent.update_q_table(reward=reward, action=action, nxt_state=nxt_state)\n",
    "        \n",
    "        if nxt_state in ['win']:\n",
    "            is_terminated = True\n",
    "        path(Agent.state, is_terminated)\n",
    "        Agent.state = nxt_state\n",
    "        count +=1\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    print('\\n Episode. '+str(episode)+' finished ... ,total step : '+str(count))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.99      , -1.23532666, -1.99      , -1.25116878],\n",
       "        [-1.        , -0.87210106, -0.9793621 , -0.91463446],\n",
       "        [-1.        , -0.60089642, -0.59368384, -0.63343621],\n",
       "        [-1.        ,  1.92466942, -0.34117063, -0.361     ],\n",
       "        [-1.        , -0.2881    , -0.20890217, -1.        ]],\n",
       "\n",
       "       [[-0.9793621 , -0.85439372, -1.99      , -0.92629492],\n",
       "        [-0.778069  , -0.83099521, -0.947359  , -0.80238078],\n",
       "        [-0.5878    , -0.54407539, -0.56804491, -0.56387341],\n",
       "        [-0.28      , 18.83384477, -0.39955041, -0.1       ],\n",
       "        [-0.28      , -0.19      , -0.10994851, -1.        ]],\n",
       "\n",
       "       [[-0.58246453, -0.5757067 , -1.        , -0.57537865],\n",
       "        [-0.60490315, -0.5273659 , -0.5878    , -0.469702  ],\n",
       "        [-0.34075   , -0.2881    , -0.34075   , -0.271     ],\n",
       "        [-0.2233    , 61.2579511 ,  0.        ,  0.        ],\n",
       "        [-0.109     , 10.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.31167748, -0.442     , -1.        ,  1.75248254],\n",
       "        [-0.110539  , -0.19      , -0.442     , 17.97291035],\n",
       "        [-0.10981   , -0.1       , -0.109     , 61.2579511 ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.2071    , -1.        , -1.        , -0.2962    ],\n",
       "        [-0.20890217, -1.99      , -0.28      , -0.19      ],\n",
       "        [-0.1098829 , -1.        , -0.109     , -0.1       ],\n",
       "        [10.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29e949a2a32639188ce000b963d9a6351a77dfe19aba7adb39ffce4ed6e0154f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tf25')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
