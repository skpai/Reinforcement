{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcacf983a30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcac7858be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 5), (5, 4), (5, 6), (6, 5)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_positions(position):\n",
    "    x, y = position\n",
    "    xvals=x-1, x, x+1\n",
    "    yvals=y-1, y, y+1\n",
    "    valid=[(i,j) for i in xvals for j in yvals if (((i-x)*(j-y)==0)) and ((i,j)!=(x,y))]\n",
    "    return valid\n",
    "get_valid_positions((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 4), (4, 5), (4, 6), (5, 4), (5, 5), (5, 6), (6, 4), (6, 5), (6, 6)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_valid_positions((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-b30a14958045>:5: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix=np.random.random((5,5))\n",
    "fig, ax= plt.subplots()\n",
    "ax.pcolor(matrix, edgecolors='w', linewidths=2)\n",
    "ax.scatter(3-0.5,3-0.5, s=400,c='b' )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((nS, nA))\n",
    "    games_reward = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        tot_rew = 0\n",
    "\n",
    "\n",
    "        action = eps_greedy(Q, state, eps) \n",
    "\n",
    "        while not done:\n",
    "            next_state, rew, done, _ = env.step(action) \n",
    "\n",
    "            # choose the next action (needed for the SARSA update)\n",
    "            next_action = eps_greedy(Q, next_state, eps) \n",
    "            # SARSA update\n",
    "            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            tot_rew += rew\n",
    "            if done:\n",
    "                games_reward.append(tot_rew)\n",
    "\n",
    "        # Test the policy every 300 episodes and print the results\n",
    "        if (ep % 300) == 0:\n",
    "            test_rew = run_episodes(env, Q, 1000)\n",
    "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
    "            test_rewards.append(test_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the different parameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    " \n",
    "#Initializing the Q-matrix\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "#Function to choose the next action\n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    " \n",
    "#Function to learn the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)\n",
    "\n",
    "#Initializing the reward\n",
    "reward=0\n",
    " \n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "    t = 0\n",
    "    state1 = env.reset()\n",
    "    action1 = choose_action(state1)\n",
    " \n",
    "    while t < max_steps:\n",
    "        #Visualizing the training\n",
    "        env.render()\n",
    "         \n",
    "        #Getting the next state\n",
    "        state2, reward, done, info = env.step(action1)\n",
    " \n",
    "        #Choosing the next action\n",
    "        action2 = choose_action(state2)\n",
    "         \n",
    "        #Learning the Q-value\n",
    "        update(state1, state2, reward, action1, action2)\n",
    " \n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "         \n",
    "        #Updating the respective vaLues\n",
    "        t += 1\n",
    "        reward += 1\n",
    "         \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        self.q = {}\n",
    "\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "        # return self.q.get((state, action), 1.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:        \n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            # In case there're several state-action max values \n",
    "            # we select a random one among them\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "import math\n",
    "def ff(f,n):\n",
    "    fs = \"{:f}\".format(f)\n",
    "    if len(fs) < n:\n",
    "        return (\"{:\"+n+\"s}\").format(fs)\n",
    "    else:\n",
    "        return fs[:n]\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = {}\n",
    "\n",
    "epsilon = epsilon  # exploration constant\n",
    "alpha = alpha      # discount constant\n",
    "gamma = gamma\n",
    "actions = actions\n",
    "\n",
    "def getQ(state, action):\n",
    "    return q.get((state, action), 0.0)\n",
    "    # return q.get((state, action), 1.0)\n",
    "\n",
    "def learnQ(state, action, reward, value):\n",
    "    '''\n",
    "    Q-learning:        \n",
    "        Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "    '''\n",
    "    oldv = q.get((state, action), None)\n",
    "    if oldv is None:\n",
    "        q[(state, action)] = reward\n",
    "    else:\n",
    "        q[(state, action)] = oldv + alpha * (value - oldv)\n",
    "\n",
    "def chooseAction(state):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.choice(actions)\n",
    "    else:\n",
    "        q = [getQ(state, a) for a in actions]\n",
    "        maxQ = max(q)\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values \n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = actions[i]\n",
    "    return action\n",
    "\n",
    "def learn(state1, action1, reward, state2):\n",
    "    maxqnew = max([getQ(state2, a) for a in actions])\n",
    "    learnQ(state1, action1, reward, reward + gamma*maxqnew)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fbac483a4a1edeb8f7db3ca98d3cb42b3995d9dd792bf420e1e81a47d25d7f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('keras_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
